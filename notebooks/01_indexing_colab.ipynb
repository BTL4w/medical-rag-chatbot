{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01_indexing_colab\n",
        "Indexing pipeline for YouMed articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\study\\code\\.venv\\lib\\site-packages (4.36.0)\n",
            "Requirement already satisfied: qdrant-client in c:\\study\\code\\.venv\\lib\\site-packages (1.7.0)\n",
            "Requirement already satisfied: pinecone-client in c:\\study\\code\\.venv\\lib\\site-packages (2.2.4)\n",
            "Requirement already satisfied: torch in c:\\study\\code\\.venv\\lib\\site-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: langchain in c:\\study\\code\\.venv\\lib\\site-packages (0.1.0)\n",
            "Requirement already satisfied: rank-bm25 in c:\\study\\code\\.venv\\lib\\site-packages (0.2.2)\n",
            "Requirement already satisfied: filelock in c:\\study\\code\\.venv\\lib\\site-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\study\\code\\.venv\\lib\\site-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\study\\code\\.venv\\lib\\site-packages (from transformers) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\study\\code\\.venv\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\study\\code\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\study\\code\\.venv\\lib\\site-packages (from transformers) (2025.9.18)\n",
            "Requirement already satisfied: requests in c:\\study\\code\\.venv\\lib\\site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\study\\code\\.venv\\lib\\site-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\study\\code\\.venv\\lib\\site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\study\\code\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\study\\code\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\study\\code\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.15.0)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in c:\\study\\code\\.venv\\lib\\site-packages (from qdrant-client) (1.76.0)\n",
            "Requirement already satisfied: grpcio-tools>=1.41.0 in c:\\study\\code\\.venv\\lib\\site-packages (from qdrant-client) (1.76.0)\n",
            "Requirement already satisfied: httpx>=0.14.0 in c:\\study\\code\\.venv\\lib\\site-packages (from httpx[http2]>=0.14.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in c:\\study\\code\\.venv\\lib\\site-packages (from qdrant-client) (2.10.1)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in c:\\study\\code\\.venv\\lib\\site-packages (from qdrant-client) (2.5.0)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.26.14 in c:\\study\\code\\.venv\\lib\\site-packages (from qdrant-client) (1.26.20)\n",
            "Requirement already satisfied: pywin32>=226 in c:\\study\\code\\.venv\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (311)\n",
            "Requirement already satisfied: loguru>=0.5.0 in c:\\study\\code\\.venv\\lib\\site-packages (from pinecone-client) (0.7.3)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in c:\\study\\code\\.venv\\lib\\site-packages (from pinecone-client) (2.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\study\\code\\.venv\\lib\\site-packages (from pinecone-client) (2.9.0.post0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\study\\code\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\study\\code\\.venv\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\study\\code\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\study\\code\\.venv\\lib\\site-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\study\\code\\.venv\\lib\\site-packages (from langchain) (3.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\study\\code\\.venv\\lib\\site-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\study\\code\\.venv\\lib\\site-packages (from langchain) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\study\\code\\.venv\\lib\\site-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.9 in c:\\study\\code\\.venv\\lib\\site-packages (from langchain) (0.0.10)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in c:\\study\\code\\.venv\\lib\\site-packages (from langchain) (0.1.23)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in c:\\study\\code\\.venv\\lib\\site-packages (from langchain) (0.0.87)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\study\\code\\.venv\\lib\\site-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\study\\code\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\study\\code\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\study\\code\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\study\\code\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\study\\code\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\study\\code\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.4.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\study\\code\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\study\\code\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\study\\code\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\study\\code\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (3.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3 in c:\\study\\code\\.venv\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\study\\code\\.venv\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\study\\code\\.venv\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in c:\\study\\code\\.venv\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\study\\code\\.venv\\lib\\site-packages (from pydantic>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.1 in c:\\study\\code\\.venv\\lib\\site-packages (from pydantic>=1.10.8->qdrant-client) (2.14.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\study\\code\\.venv\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\study\\code\\.venv\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\study\\code\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\study\\code\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.1.0)\n",
            "Requirement already satisfied: protobuf<7.0.0,>=6.31.1 in c:\\study\\code\\.venv\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (6.33.4)\n",
            "Requirement already satisfied: setuptools in c:\\study\\code\\.venv\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (80.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\study\\code\\.venv\\lib\\site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\study\\code\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in c:\\study\\code\\.venv\\lib\\site-packages (from httpx[http2]>=0.14.0->qdrant-client) (4.3.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\study\\code\\.venv\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in c:\\study\\code\\.venv\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: colorama>=0.3.4 in c:\\study\\code\\.venv\\lib\\site-packages (from loguru>=0.5.0->pinecone-client) (0.4.6)\n",
            "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\study\\code\\.venv\\lib\\site-packages (from loguru>=0.5.0->pinecone-client) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\study\\code\\.venv\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\study\\code\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\study\\code\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers qdrant-client pinecone-client torch langchain rank-bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "add5d736",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import io\n",
        "\n",
        "sys.path.append(os.path.abspath('../src'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chunks: 55973\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from core.chunking import MarkdownChunker\n",
        "\n",
        "chunker = MarkdownChunker()\n",
        "chunks = []\n",
        "with open(\"../data/processed/youmed_articles.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        doc = json.loads(line)\n",
        "        chunks.extend(chunker.chunk_document(doc[\"content\"], doc[\"metadata\"]))\n",
        "\n",
        "print(f\"Total chunks: {len(chunks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\STUDY\\code\\.venv\\lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b206ebcc01e54361b2a673f6c601c846",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\STUDY\\code\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad26e034ec124a9e9e431c34885dc791",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\STUDY\\code\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PC\\.cache\\huggingface\\hub\\models--BAAI--bge-m3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c1af07b574e44edaf1c39a216677128",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ddc6d0ba52a465283a0730187074bd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "feb45dd4c293447e8c26701a9433f6c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e4b20aa453246c8b65024fe063c2693",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\STUDY\\code\\.venv\\lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23ca575922c44634afc88c3d8b46895b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBAAI/bge-m3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBAAI/bge-m3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21membed_all\u001b[39m(texts, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m):\n\u001b[0;32m      8\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[1;32mc:\\STUDY\\code\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:2432\u001b[0m, in \u001b[0;36mPreTrainedModel.cuda\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2427\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `cuda()` is not supported for `4-bit` or `8-bit` quantized models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2430\u001b[0m     )\n\u001b[0;32m   2431\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcuda(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\STUDY\\code\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1084\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \n\u001b[0;32m   1070\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1084\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\STUDY\\code\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 930\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    934\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    940\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    941\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\STUDY\\code\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 930\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    934\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    940\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    941\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\STUDY\\code\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:957\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 957\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    958\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
            "File \u001b[1;32mc:\\STUDY\\code\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1084\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \n\u001b[0;32m   1070\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1084\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32mc:\\STUDY\\code\\.venv\\lib\\site-packages\\torch\\cuda\\__init__.py:403\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    401\u001b[0m     )\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m     )\n",
            "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
        "model = AutoModel.from_pretrained(\"BAAI/bge-m3\").cuda()\n",
        "\n",
        "def embed_all(texts, batch_size=64):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs).last_hidden_state\n",
        "            mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
        "            pooled = (outputs * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-9)\n",
        "            pooled = torch.nn.functional.normalize(pooled, p=2, dim=1)\n",
        "        embeddings.append(pooled.cpu())\n",
        "    return torch.cat(embeddings, dim=0)\n",
        "\n",
        "texts = [c.enriched_content for c in chunks]\n",
        "embeddings = embed_all(texts)\n",
        "print(embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.db.vector_store import QdrantStore\n",
        "\n",
        "store = QdrantStore(url=\"http://localhost:6333\")\n",
        "store.create_collection(name=\"youmed_articles\", dimension=embeddings.shape[1])\n",
        "store.upsert(chunks, embeddings.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BM25 index saved\n"
          ]
        }
      ],
      "source": [
        "from core.retriever import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever(chunks)\n",
        "bm25_retriever.save(\"models/bm25_index.pkl\")\n",
        "print(\"BM25 index saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cacf364f",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
