{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01_indexing_colab\n",
        "Indexing pipeline for YouMed articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers qdrant-client pinecone-client torch langchain rank-bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from src.core.chunking import MarkdownChunker\n",
        "\n",
        "chunker = MarkdownChunker()\n",
        "chunks = []\n",
        "with open(\"data/processed/youmed_articles_test.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        doc = json.loads(line)\n",
        "        chunks.extend(chunker.chunk_document(doc[\"content\"], doc[\"metadata\"]))\n",
        "\n",
        "print(f\"Total chunks: {len(chunks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
        "model = AutoModel.from_pretrained(\"BAAI/bge-m3\").cuda()\n",
        "\n",
        "def embed_all(texts, batch_size=64):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs).last_hidden_state\n",
        "            mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
        "            pooled = (outputs * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-9)\n",
        "            pooled = torch.nn.functional.normalize(pooled, p=2, dim=1)\n",
        "        embeddings.append(pooled.cpu())\n",
        "    return torch.cat(embeddings, dim=0)\n",
        "\n",
        "texts = [c.enriched_content for c in chunks]\n",
        "embeddings = embed_all(texts)\n",
        "print(embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.db.vector_store import QdrantStore\n",
        "\n",
        "store = QdrantStore(url=\"http://localhost:6333\")\n",
        "store.create_collection(name=\"youmed_articles\", dimension=embeddings.shape[1])\n",
        "store.upsert(chunks, embeddings.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.core.retriever import BM25Retriever\n",
        "import pickle\n",
        "\n",
        "bm25_retriever = BM25Retriever(chunks)\n",
        "with open(\"models/bm25_index.pkl\", \"wb\") as f:\n",
        "    pickle.dump(bm25_retriever, f)\n",
        "print(\"BM25 index saved\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
